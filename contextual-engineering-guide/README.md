# 使用上下文工程优化 LangChain AI 代理

本项目聚焦于 **LangChain**，一个强大的、用于构建基于大型语言模型（LLM）的应用程序的开源框架。我们强调通过**上下文工程（Contextual Engineering）** 来显著优化 AI 代理（Agents）的表现。

![Context Engineering](https://cdn-images-1.medium.com/max/1500/1*sCTOzjG6KP7slQuxLZUtNg.png)

---

## 核心框架：LangChain 与 LangGraph

### LangChain：构建 LLM 应用的基础

**LangChain** 是一个旨在简化 LLM 应用开发的框架。它通过提供模块化的组件，帮助开发者轻松地将 LLM 与外部数据源、计算工具和内存系统连接起来，构建复杂的应用程序。其核心组件包括：

*   **链 (Chains):** 将 LLM 调用与其他组件按顺序组合起来，形成一个连贯的工作流。
*   **工具 (Tools):** 允许 LLM 与外部世界交互的功能接口，例如执行代码、调用 API 或进行网络搜索。
*   **内存 (Memory):** 使 LLM 能够记住之前的交互内容，保持对话的连续性。
*   **代理 (Agents):** LangChain 的核心组件之一。代理是一个智能实体，它使用 LLM 作为其“大脑”，能够根据用户的输入动态地选择和调用合适的工具来完成任务。

### LangGraph：实现循环与状态管理

**为什么需要 LangGraph？**

标准的 LangChain “链”本质上是**有向无环图（DAG）**，数据流是单向的，这对于简单的任务非常有效。然而，要构建真正智能的、能够自主决策的代理，我们需要更强大的控制流，比如**循环（Cycles）**。

代理在执行任务时，**经常需要在一个“思考-行动”的循环中迭代**：接收用户输入 -> 思考下一步该用什么工具 -> 调用工具 -> 观察结果 -> 根据结果再次思考... 这个过程无法用简单的线性链条来表示。

**LangGraph** 正是为解决这个问题而生。它是一个建立在 LangChain 之上的库，专门用于构建**有状态的、可包含循环**的多步应用程序。它将应用流程建模为**图（Graph）**，其中：

*   **节点 (Nodes):** 代表一个计算步骤（可以是一个函数、一个工具调用或一个 LLM 调用）。
*   **边 (Edges):** 连接节点，定义了工作流的方向。关键在于，这些边可以是**条件性的（Conditional）**，允许根据节点的输出结果，决定下一步该走向哪个节点，从而轻松实现循环和复杂的分支逻辑。

简单来说，**LangChain 提供了构建块，而 LangGraph 提供了将这些构建块组合成能够进行复杂、循环和有状态推理的代理的“粘合剂”**。因此，在本项目中，大量使用 了LangGraph 来实现高级的代理行为。

## 上下文工程：优化 AI 代理的关键

随着 AI 代理处理的任务日益复杂，仅仅依赖于优秀的提示词（Prompt）已不足以保证高质量的输出。**上下文工程**应运而生，它是一种更全面、更结构化的方法，旨在通过精心设计和管理提供给 LLM 的完整上下文信息，从而提升其决策能力、输出质量和任务执行效率。

一个经过良好上下文工程设计的代理，能够更清晰地理解任务目标、更准确地使用工具、并更有效地利用历史信息，最终表现得更智能、更可靠。

![Different Context Types](https://cdn-images-1.medium.com/max/1000/1*kMEQSslFkhLiuJS8-WEMIg.png)

## 上下文工程的核心策略

上下文工程涵盖了多个关键策略，这些策略共同作用，为 AI 代理构建了一个高效的工作环境：

1.  **提示优化 (Prompt Optimization):**
    *   设计清晰、明确的系统提示（System Prompt），为代理设定角色、目标和行为准则。
    *   使用少量示例（Few-shot Examples）来引导模型产生符合特定格式或风格的输出。

2.  **工具选择与集成 (Tool Selection & Integration):**
    *   为代理配备完成任务所需的最少、最精确的工具集，避免因工具过多或描述模糊导致的“选择困难”。
    *   优化工具的描述，使其易于被 LLM 理解和调用。

3.  **上下文内存管理 (Contextual Memory Management):**
    *   设计高效的短期记忆（如 Scratchpad）和长期记忆机制。
    *   在处理长对话或复杂任务时，对历史信息进行压缩或摘要，以避免超出 LLM 的上下文窗口限制。

4.  **动态上下文注入 (Dynamic Context Injection):**
    *   根据任务的当前状态，动态地从外部知识库（如向量数据库）中检索并注入最相关的信息（RAG - Retrieval-Augmented Generation）。
    *   在多代理系统中，有选择地传递上下文，实现任务的隔离与协作。

5.  **错误处理与重试逻辑 (Error Handling & Retry Logic):**
    *   为代理设计稳健的错误处理机制，当工具调用失败或遇到意外情况时，能够自我修正或请求帮助。

![Categories of Context Engineering](https://cdn-images-1.medium.com/max/2600/1*CacnXVAI6wR4eSIWgnZ9sg.png)

## 在 LangChain 中实践上下文工程

LangChain 的模块化设计为实践上述上下文工程策略提供了天然的支持：

*   **提示优化**可以通过 `PromptTemplate` 和 `ChatPromptTemplate` 来实现。
*   **工具集成**是 LangChain 的核心功能，可以轻松创建和绑定自定义工具。
*   **内存管理**由多种 `Memory` 类支持，并可以与 LangGraph 结合实现更复杂的暂存盘（Scratchpad）逻辑。下面将详细介绍。
*   **动态上下文注入**可以通过构建 RAG 链，或在 LangGraph 中设计专门的节点来实现。
*   **错误处理**可以在代理的执行逻辑中自定义，LangGraph 提供了足够的灵活性来构建包含重试和修正循环的复杂图。

### LangChain 中的内存管理

**内存管理是上下文工程的支柱**，它直接关系到代理能否在长程任务中保持上下文连贯性，同时避免超出模型的Token限制。LangChain 提供了丰富的内存类来管理短期和长期记忆。

#### 短期记忆优化 (Short-Term Memory)

短期记忆主要处理当前单次会话（Session）中的信息。核心挑战是在保持对话流畅性的同时，防止上下文窗口溢出。

*   **`ConversationBufferMemory`**: 最基础的内存。它按原样存储所有历史对话消息，简单直接，但上下文会随对话增长而线性膨胀，很快会达到Token上限。
*   **`ConversationBufferWindowMemory`**: 对基础版的改进。它只保留最近的 `k` 轮对话，通过滑动窗口机制来控制上下文长度，是一种简单有效的短期记忆截断策略。
*   **`ConversationTokenBufferMemory`**: 更智能的缓冲。它不按对话轮数，而是按**Token数量**来限制内存大小。当历史消息的总Token数超过设定阈值时，它会从最早的消息开始移除，控制更精确。

#### 长期记忆与短期记忆的结合优化

为了让代理能够跨会话学习并处理更复杂的任务，需要将短期记忆与长期记忆结合，并通过摘要或检索的方式进行优化。

*   **`ConversationSummaryMemory`**: **摘要化策略**。当对话历史变长时，它会调用一个LLM，将早期的对话内容**动态地总结成摘要**，然后用这个摘要来代替冗长的原文。这极大地压缩了Token占用，保留了核心信息，是处理超长对话的有效手段。
*   **`ConversationSummaryBufferMemory`**: **混合策略**。它结合了缓冲和摘要的优点：在内存中保留最近的对话原文（缓冲），同时将更早的对话动态地生成摘要。这样既保证了近期对话的完整性，又实现了对远期历史的有效压缩。
*   **`VectorStoreRetrieverMemory`**: **检索式策略**。它将每一段对话（或实体信息）作为文档存入**向量数据库**。在需要时，它不是加载全部历史，而是根据当前输入的语义，从向量库中**检索最相关的历史片段**并注入上下文。这是一种高效的长期记忆提取方式，非常适合需要从大量历史信息中查找特定知识的场景。
*   **`EntityMemory`**: **实体为中心**。它专门关注对话中提到的**实体（如人名、地名、概念）**，并围绕这些实体来存储和回忆信息。它能记住关于特定实体的关键事实，非常适合需要深度事实跟踪的问答或分析任务。

#### 结合外部存储优化内存管理

LangChain的内存机制可以、而且强烈推荐与**外部存储系统**结合，以实现持久化、可扩展和更高效的记忆管理。

*   **与缓存系统结合 (如 Redis, Memcached):**
    *   **目的**: 提升重复查询的响应速度，降低LLM调用成本。
    *   **实现**: 可以使用如 `RedisCache` 或自定义缓存逻辑，将LLM的请求（Prompt）和响应（Completion）作为键值对存入缓存。在下一次遇到完全相同的请求时，直接从缓存中返回结果，无需再次调用昂贵的LLM API。
    *   **场景**: 对于知识问答、固定指令转换等重复性高的任务，缓存能带来巨大的性能提升和成本节约。

*   **与数据库结合 (如 SQL, NoSQL, 向量数据库):**
    *   **目的**: 实现真正的**持久化长期记忆**。
    *   **实现**: `ChatMessageHistory` 类可以轻松地与外部数据库集成。例如，`RedisChatMessageHistory` 或 `MongoDBChatMessageHistory` 可以将对话历史直接存入对应的数据库中。对于 `VectorStoreRetrieverMemory`，其后端本身就是一个向量数据库（如Chroma, FAISS, Pinecone），实现了记忆的持久化和高效检索。
    *   **优势**: 这使得代理的记忆不再局限于单次运行的内存中，而是可以跨应用、跨服务器、长期保存和复用，为构建企业级的、有持续学习能力的AI应用奠定了基础。

通过组合使用这些内存类，并将其与外部存储系统集成，开发者可以为AI代理构建一个强大、灵活且可扩展的记忆系统，从容应对各种复杂任务。

## 关于此 Jupyter Notebook

本仓库中的 Jupyter Notebook (`Contextual_Engineering_in_LangChain.ipynb`) 提供了上述概念的**实用代码示例**。

*   了解如何使用 LangChain 和 LangGraph 构建代理。
*   实践不同的上下文工程技术，如内存管理、工具选择和上下文压缩。
*   通过具体的例子，直观地感受上下文工程如何提升 AI 代理的性能和效率。

可以动手运行代码，并尝试修改和扩展这些示例，以加深对 LangChain 和上下文工程的理解。
