diff --git a/torch/csrc/distributed/c10d/ProcessGroupMPI.cpp b/torch/csrc/distributed/c10d/ProcessGroupMPI.cpp
index 91e9f938f1d..a53ef20a10f 100644
--- a/torch/csrc/distributed/c10d/ProcessGroupMPI.cpp
+++ b/torch/csrc/distributed/c10d/ProcessGroupMPI.cpp
@@ -50,11 +50,12 @@ std::map<at::ScalarType, MPI_Datatype> mpiDatatype = {
 bool cudaAwareMpiCheck() {
 // Run time check
 #if defined(MPIX_CUDA_AWARE_SUPPORT)
-  if (MPIX_Query_cuda_support() == 1) {
-    return true;
-  } else {
-    return false;
-  }
+  // if (MPIX_Query_cuda_support() == 1) {
+  //   return true;
+  // } else {
+  //   return false;
+  // }
+  return true; // The MPIX_Query_cuda_support() check is owerwritten with always true value, as for some reason it fails afer hipified for 
 #else // !defined(MPIX_CUDA_AWARE_SUPPORT)
   return false;
 #endif // MPIX_CUDA_AWARE_SUPPORT
diff --git a/torch/csrc/distributed/c10d/ProcessGroupUCC.cpp b/torch/csrc/distributed/c10d/ProcessGroupUCC.cpp
index d52adada458..113d7f750fa 100644
--- a/torch/csrc/distributed/c10d/ProcessGroupUCC.cpp
+++ b/torch/csrc/distributed/c10d/ProcessGroupUCC.cpp
@@ -1,6 +1,6 @@
 #ifdef USE_C10D_UCC
 
-#include <ATen/cuda/nvrtc_stub/ATenNVRTC.h>
+// #include <ATen/cuda/nvrtc_stub/ATenNVRTC.h>
 #include <torch/csrc/distributed/c10d/ProcessGroupUCC.hpp>
 #include <torch/csrc/distributed/c10d/UCCTracing.hpp>
 #include <torch/csrc/distributed/c10d/UCCUtils.hpp>
@@ -528,13 +528,14 @@ void Comm::progress_loop() {
 #ifdef USE_CUDA
     if ((!device_set) && (cuda_device_index != TORCH_UCC_DEVICE_NOT_SET)) {
       c10::cuda::set_device(cuda_device_index);
-      CUcontext pctx = nullptr;
-      at::globalContext().getNVRTC().cuCtxGetCurrent(&pctx);
-      if (C10_UNLIKELY(!pctx)) {
-        at::globalContext().getNVRTC().cuDevicePrimaryCtxRetain(
-            &pctx, cuda_device_index);
-        at::globalContext().getNVRTC().cuCtxSetCurrent(pctx);
-      }
+      // This is not supported when building for AMD NAVI 12 arch
+      // CUcontext pctx = nullptr;
+      // at::globalContext().getNVRTC().cuCtxGetCurrent(&pctx);
+      // if (C10_UNLIKELY(!pctx)) {
+      //   at::globalContext().getNVRTC().cuDevicePrimaryCtxRetain(
+      //       &pctx, cuda_device_index);
+      //   at::globalContext().getNVRTC().cuCtxSetCurrent(pctx);
+      // }
       device_set = true;
     }
 #endif
diff --git a/torch/utils/hipify/cuda_to_hip_mappings.py b/torch/utils/hipify/cuda_to_hip_mappings.py
index 75dfd0ef316..cbbc84275ea 100644
--- a/torch/utils/hipify/cuda_to_hip_mappings.py
+++ b/torch/utils/hipify/cuda_to_hip_mappings.py
@@ -8402,6 +8402,17 @@ CUDA_SPECIAL_MAP = collections.OrderedDict(
         # gesvdj SetXXX
         ('cusolverDnXgesvdjSetTolerance', ('hipsolverDnXgesvdjSetTolerance', CONV_MATH_FUNC, API_SPECIAL)),
         ('cusolverDnXgesvdjSetMaxSweeps', ('hipsolverDnXgesvdjSetMaxSweeps', CONV_MATH_FUNC, API_SPECIAL)),
+        
+        # Adding missing mappings to hipify
+        # UCC
+        ('UCS_MEMORY_TYPE_CUDA', ('UCS_MEMORY_TYPE_ROCM', CONV_NUMERIC_LITERAL, API_SPECIAL)),
+        ('UCC_MEMORY_TYPE_CUDA', ('UCC_MEMORY_TYPE_ROCM', CONV_NUMERIC_LITERAL, API_SPECIAL)),
+        ('UCC_EE_CUDA_STREAM',   ('UCC_EE_ROCM_STREAM', CONV_NUMERIC_LITERAL, API_SPECIAL)),
+        
+        # MPI
+        ('MPIX_CUDA_AWARE_SUPPORT',   ('MPIX_ROCM_AWARE_SUPPORT', CONV_NUMERIC_LITERAL, API_SPECIAL)),
+        ('cudaAwareMpiCheck', ('rocmAwareMpiCheck', CONV_SPECIAL_FUNC, API_SPECIAL)),
+        ('MPIX_Query_cuda_support', ('MPIX_Query_rocm_support', CONV_DEVICE_FUNC, API_SPECIAL)),
     ]
 )
 
