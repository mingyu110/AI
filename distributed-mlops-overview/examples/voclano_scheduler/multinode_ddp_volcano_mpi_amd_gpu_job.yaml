apiVersion: v1
data:
  main.py: |
    import torch
    import torch.nn.functional as F
    from torch.utils.data import Dataset, DataLoader

    import torch.multiprocessing as mp
    from torch.utils.data.distributed import DistributedSampler
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torch.distributed import init_process_group, destroy_process_group
    import os
    from time import perf_counter


    LOCAL_RANK = int(os.environ["OMPI_COMM_WORLD_LOCAL_RANK"])
    WORLD_SIZE = int(os.environ["OMPI_COMM_WORLD_SIZE"])
    WORLD_RANK = int(os.environ["OMPI_COMM_WORLD_RANK"])


    class MyTrainDataset(Dataset):
        def __init__(self, size):
            self.size = size
            self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)]

        def __len__(self):
            return self.size

        def __getitem__(self, index):
            return self.data[index]


    def ddp_setup(backend: str = "nccl"):
        torch.cuda.set_device(LOCAL_RANK)
        init_process_group(backend=backend, rank=WORLD_RANK, world_size=WORLD_SIZE)


    class Trainer:
        def __init__(
            self,
            model: torch.nn.Module,
            train_data: DataLoader,
            optimizer: torch.optim.Optimizer,
            save_every: int,
            snapshot_path: str,
        ) -> None:
            self.local_rank = LOCAL_RANK
            self.global_rank = WORLD_RANK
            self.model = model.to(self.local_rank)
            self.train_data = train_data
            self.optimizer = optimizer
            self.save_every = save_every
            self.epochs_run = 0
            self.snapshot_path = snapshot_path
            if os.path.exists(snapshot_path):
                print("Loading snapshot")
                self._load_snapshot(snapshot_path)

            self.model = DDP(self.model, device_ids=[self.local_rank])

        def _load_snapshot(self, snapshot_path):
            loc = f"cuda:{self.local_rank}"
            snapshot = torch.load(snapshot_path, map_location=loc)
            self.model.load_state_dict(snapshot["MODEL_STATE"])
            self.epochs_run = snapshot["EPOCHS_RUN"]
            print(f"Resuming training from snapshot at Epoch {self.epochs_run}")

        def _run_batch(self, source, targets):
            self.optimizer.zero_grad()
            output = self.model(source)
            loss = F.cross_entropy(output, targets)
            loss.backward()
            self.optimizer.step()

        def _run_epoch(self, epoch):
            b_sz = len(next(iter(self.train_data))[0])
            print(
                f"[GPU{self.global_rank}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}"
            )
            self.train_data.sampler.set_epoch(epoch)
            for source, targets in self.train_data:
                source = source.to(self.local_rank)
                targets = targets.to(self.local_rank)
                self._run_batch(source, targets)

        def _save_snapshot(self, epoch):
            snapshot = {
                "MODEL_STATE": self.model.module.state_dict(),
                "EPOCHS_RUN": epoch,
            }
            torch.save(snapshot, self.snapshot_path)
            print(f"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}")

        def train(self, max_epochs: int):
            for epoch in range(self.epochs_run, max_epochs):
                self._run_epoch(epoch)
                if self.local_rank == 0 and epoch % self.save_every == 0:
                    self._save_snapshot(epoch)


    def load_train_objs():
        train_set = MyTrainDataset(2048)  # load your dataset
        model = torch.nn.Linear(20, 1)  # load your model
        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
        return train_set, model, optimizer


    def prepare_dataloader(dataset: Dataset, batch_size: int):
        return DataLoader(
            dataset,
            batch_size=batch_size,
            pin_memory=True,
            shuffle=False,
            sampler=DistributedSampler(dataset),
        )


    def main(
        save_every: int,
        total_epochs: int,
        batch_size: int,
        backend: str,
        snapshot_path: str = "snapshot.pt",
    ):
        ddp_setup(backend)
        dataset, model, optimizer = load_train_objs()
        train_data = prepare_dataloader(dataset, batch_size)
        trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)
        t1_start = perf_counter()
        trainer.train(total_epochs)
        t1_stop = perf_counter()
        print(f"Training took {(t1_stop-t1_start)} s")
        destroy_process_group()


    if __name__ == "__main__":
        import argparse

        parser = argparse.ArgumentParser(description="simple distributed training job")
        parser.add_argument(
            "total_epochs", type=int, help="Total epochs to train the model"
        )
        parser.add_argument("save_every", type=int, help="How often to save a snapshot")
        parser.add_argument(
            "--batch_size",
            default=32,
            type=int,
            help="Input batch size on each device (default: 32)",
        )
        parser.add_argument(
            "--backend",
            choices=["nccl", "ucc", "mpi"],
            type=str,
            default="nccl",
            help="Backend type",
        )
        args = parser.parse_args()

        main(args.save_every, args.total_epochs, args.batch_size, args.backend)
kind: ConfigMap
metadata:
  name: training-script-config
  namespace: distributed-training

---
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: mpi-training-none
  namespace: distributed-training
spec:
  minAvailable: 3
  plugins:
    ssh: []
    svc: []
  schedulerName: volcano
  tasks:
    - name: mpimaster
      policies:
        - action: CompleteJob
          event: TaskCompleted
      replicas: 1
      template:
        spec:
          containers:
            - command:
                - /bin/bash
                - -c
                - |
                  mkdir -p /var/run/sshd; /usr/sbin/sshd; 
                  MPI_HOST=`cat /etc/volcano/mpiworker.host | tr "\n" ","`; 
                  MASTER_ADDR=$(awk 'NR==1 {print $1}' /etc/volcano/mpiworker.host);
                  NUM_WORKERS=$(($(echo ${MPI_HOST} | tr -cd ',' | wc -c) + 1));
                  mpirun -np ${NUM_WORKERS} --allow-run-as-root --host ${MPI_HOST} \
                    -x MASTER_ADDR=${MASTER_ADDR} -x MASTER_PORT=29603 \
                    /opt/conda/envs/py_3.12/bin/python /mpijob/main.py 1000 1000 --batch_size 500
              image: docker.io/rafalsiwek/opmpi_ucx_simple:1.0_base
              name: mpimaster
              ports:
                - containerPort: 22
                  name: mpijob-port
          restartPolicy: OnFailure
    - name: mpiworker
      replicas: 2
      template:
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: NodeGroupType
                        operator: In
                        values:
                          - training-operator-job-amd-gpu
          containers:
            - command:
                - /bin/bash
                - -c
                - mkdir -p /var/run/sshd; /usr/sbin/sshd -D;
              image: docker.io/rafalsiwek/g4ad_distributed_ml:1.0_pytorch_2.5.1
              name: mpiworker
              ports:
                - containerPort: 22
                  name: mpijob-port
                - containerPort: 29603
                  name: torch-port
              resources:
                limits:
                  amd.com/gpu: 1
              volumeMounts:
                - mountPath: /mpijob
                  name: script-volume
          restartPolicy: OnFailure
          tolerations:
            - effect: NoSchedule
              key: training-operator-job-gpu
          volumes:
            - configMap:
                name: training-script-config
              name: script-volume
