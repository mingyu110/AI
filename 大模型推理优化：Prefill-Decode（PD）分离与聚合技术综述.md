### 大语言模型推理优化技术之：Prefill-Decode（PD）分离与聚合技术综述

#### 一、大模型推理优化背景与PD分离/聚合的兴起

在我们研发AI机器学习平台的过程中，我和团队也针对目前在AIGC领域的模型推理优化技术做了一系列的技术调研，本文是主要针对LLM大语言模型的PD推理优化的技术分析调研文档。

#### 1.1 AIGC驱动下的推理优化需求

随着生成式人工智能（AIGC）的快速发展，大型语言模型（LLM）、扩散模型（Diffusion Models）等技术在内容生成、智能对话、图像生成等领域展现出巨大潜力。从ChatGPT到Sora，再到国内的Kimi、DeepSeek等，AIGC应用的爆发推动了AI模型从实验室走向大规模商业化部署。然而，AIGC场景对模型推理性能提出了更高要求：低延迟、高吞吐量和低成本成为核心诉求。与此同时，随着模型规模的持续扩大（如千亿参数模型的普及），推理阶段的计算复杂度和资源消耗远超训练阶段，促使研究和工程实践的重点逐渐从训练优化转向推理优化。训练优化的历史与现状，**主要围绕在解决模型训练特别是超大参数模型在多机多卡集群训练过程中的“<u>内存墙</u>”、“<u>通信墙</u>”、“<u>计算墙</u>”的技术优化**：

- **训练优化框架**：过去，AI模型的优化主要集中在训练阶段，通过分布式训练框架（如DeepSpeed、Megatron-LM、Colossal-AI）实现高效并行计算。DeepSpeed通过ZeRO（Zero Redundancy Optimizer）技术减少显存冗余，支持超大规模模型训练；Megatron-LM通过张量并行（TP）和流水线并行（PP）优化多GPU训练效率；Colossal-AI则通过异构训练和内存优化降低训练成本。
- **训练优化的核心技术**：包括<u>梯度累积</u>、<u>混合精度训练</u>（如FP16/BF16）、<u>梯度检查点</u>（Checkpointing）等，这些技术显著降低了训练时间和硬件需求。然而，训练优化的成果更多服务于模型开发，而推理阶段的实时性和成本效率直接影响用户体验和商业化落地。
- **从训练到推理的转变**：训练优化关注模型收敛速度和精度，而推理优化则聚焦于服务部署中的延迟（TTFT和TPOT）、吞吐量（Goodput）以及能耗比。随着AIGC应用的普及，用户对实时交互的需求（如智能客服、实时翻译、生成式创作）使得推理优化成为当前AI技术发展的核心瓶颈。

#### 1.2 大语言模型推理的两个阶段

- **PD分离与聚合主要针对大语言模型（LLM）的推理优化**，通过解耦Prefill和Decode阶段有效提升<u>吞吐量</u>和<u>降低延迟</u>。对多模态模型和扩散模型，PD分离思想可部分适用（如将编码与生成阶段分离），但效果有限，因其推理过程（如扩散模型的迭代去噪）与LLM的Prefill-Decode框架差异较大，优化更多依赖快速采样、模型蒸馏等技术。因此，PD分离与聚合对LLM帮助最大，对多模态模型和扩散模型有一定借鉴意义但非核心优化手段。

- 大语言模型的推理过程通常分为两个主要阶段：Prefill（预填充）和Decode（解码）。这两个阶段在计算特性和资源需求上存在显著差异：

  - **Prefill阶段**：模型接收完整的输入提示（Prompt），一次性计算所有输入Token的键值缓存（KV Cache）。Prefill阶段是计算密集型（Compute-Bound），对GPU的计算能力要求较高，尤其是当输入序列较长或请求量较大时，计算压力显著增加。其性能通过**首字延迟（Time-To-First-Token, TTFT）**衡量，表示生成第一个输出Token的延迟。

  - **Decode阶段**：生成第一个Token后，模型进入Decode阶段，逐个生成后续Token。Decode阶段是访存密集型（Memory-Bound），需频繁访问KV Cache，显存带宽成为主要瓶颈。其性能通过**平均出词时间（Time-Per-Output-Token, TPOT）**衡量，表示生成每个后续Token的平均时间。

由于**Prefill和Decode阶段的资源需求差异**，传统推理架构将两者部署在同一组GPU上会导致资源竞争，影响TTFT和TPOT的平衡，进而降低整体吞吐量。

#### 1.3 PD优化的必要性

传统推理架构（即PD合并架构）将Prefill和Decode阶段运行在相同的硬件资源上，存在以下问题：

- **资源竞争**：Prefill的计算密集型特性和Decode的访存密集型特性在同一设备上运行时，容易导致计算资源或显存带宽的瓶颈。
- **并行策略限制**：在PD合并架构中，Prefill和Decode共享相同的并行策略（如数据并行DP、流水线并行PP或张量并行TP），无法针对各自特性进行独立优化。
- **性能-成本权衡**：资源利用率低下导致推理延迟增加，同时硬件成本高企，难以满足大规模部署的需求。

为了解决上述问题，业界提出了PD分离和PD聚合两种主要优化策略：

- PD分离：**将Prefill和Decode阶段解耦，分别部署在不同硬件实例上**，针对各自的计算和访存特性进行优化，以提升吞吐量和降低延迟。
- PD聚合：**在特定场景下，通过优化调度或合并部分计算逻辑**，减少Prefill和Decode之间的资源切换开销，提高整体效率。

以下将详细分析这两种策略的理论基础、技术实现及应用案例。

### 二.  PD分离技术的理论与实现

#### 2.1 PD分离的理论基础

PD分离的核心思想是将Prefill和Decode阶段的计算任务分配到不同的硬件实例上，针对各自的资源需求进行优化。其理论依据如下：

1. 计算与访存分离：
   - Prefill阶段涉及大量矩阵运算（如自注意力机制中的Q、K、V计算），对计算单元（GPU的CUDA核心）需求高。
   - Decode阶段依赖于逐Token生成，频繁访问KV Cache，对显存带宽和存储容量要求更高。
   - 通过分离，Prefill阶段可分配更多计算资源（如高性能GPU），而Decode阶段可优化显存带宽和存储效率。
2. 独立并行策略：
   - 在PD分离架构中，Prefill和Decode阶段可以采用不同的并行策略。例如，Prefill阶段可通过数据并行（DP）处理大批量请求，而Decode阶段可通过专家并行（EP）或流水线并行（PP）优化长序列生成。
3. 吞吐量与延迟优化：
   - PD分离通过解耦资源竞争，显著降低TTFT和TPOT。例如，实验表明，PD分离架构在三卡配置（2张GPU用于Prefill，1张用于Decode）下，吞吐量可达合并架构的2.1倍。

#### 2.2 PD分离的技术实现

PD分离架构的典型实现包括以下几个关键组件：

1. Prefill实例：

   - 负责处理输入Prompt，生成初始KV Cache。
   - 优化方向：增大Batch Size以提高计算效率；采用高性能GPU以应对计算密集型任务；通过数据并行（DP）或张量并行（TP）提升吞吐量。

2. Decode实例：

   - 负责逐Token生成，频繁访问KV Cache。
   - 优化方向：优化显存管理（如LRU缓存策略）；采用高带宽GPU或分布式存储以减少访存延迟；通过专家并行（EP）支持MoE模型。

3. KV Cache传输：

   - Prefill阶段生成的KV Cache需通过高速网络（如NVLink或InfiniBand）传输至Decode实例。
   - 挑战：网络传输延迟可能成为瓶颈，需要高规格网络硬件支持（如NVLink 600GB/s或更高）。

4. 调度器：

   - 引入长度预测模块，根据输入Prompt长度和Decode实例负载，动态分配任务。

   - 例如，DeepSeek通过1:10的Prefill-to-Decode GPU分配比例优化H800集群的性能。

     

#### **2.3 PD分离的开源项目与案例**

1. Mooncake：

   - 由月之暗面（Kimi）与清华大学MADSys实验室联合开发，Mooncake是基于KV Cache的PD分离推理架构。

   - 特点：通过以KV Cache为中心的设计，优化Prefill和Decode阶段的资源分配，显著提升吞吐量并降低推理成本。

   - 开源情况：2024年6月，Mooncake正式开源，吸引了阿里云、华为存储等多家机构的参与，构建了高性能推理框架的开源生态。

   - 开源地址： https://github.com/kvcache-ai/mooncake

     

2. DistServe：

   - DistServe是一个专注于PD分离的推理框架，通过解耦Prefill和Decode阶段，优化TTFT和TPOT。
   - 实验结果：在三卡配置（2张Prefill GPU，1张Decode GPU）下，DistServe的吞吐量达到10 rps（requests per second），是PD合并架构的2.1倍。
   - 开源地址：https://hao-ai-lab.github.io/blogs/distserve/

3. DeepSeek：

   - DeepSeek系列模型（如DeepSeek R1、V2、V3）广泛采用PD分离技术，通过1:10的Prefill-to-Decode GPU分配比例优化推理性能。
   - 技术亮点：结合多头潜在注意力（MLA）和多Token预测（MTP）机制，进一步提升吞吐量。

#### 2.4 PD分离的优势与挑战优势：

- 性能提升：通过独立优化Prefill和Decode阶段，显著降低TTFT和TPOT，提升整体吞吐量。
- 资源利用率：根据阶段特性分配不同类型的GPU（如高算力GPU用于Prefill，高带宽GPU用于Decode），提高硬件利用率。
- 灵活性：支持动态调整GPU数量，适应不同输入长度和输出序列的需求。

挑战：

- KV Cache传输开销：跨设备传输KV Cache需要高带宽网络支持，否则可能引入额外延迟。
- 模型副本开销：PD分离需要加载多个模型副本，增加显存占用。
- 调度复杂性：需要复杂的调度器来协调Prefill和Decode实例，增加系统设计难度。

------

### 三、PD聚合技术的理论与实现

#### 3.1 PD聚合的理论基础

与PD分离不同，PD聚合旨在通过优化调度或合并部分计算逻辑，减少Prefill和Decode阶段之间的切换开销，适用于资源受限或对延迟敏感的场景。PD聚合的核心思想包括：

- 合并计算逻辑：通过在Prefill阶段预测部分Decode行为（如多Token预测，MTP），减少后续Decode的计算量。
- 统一调度：通过智能调度器，动态平衡Prefill和Decode的资源分配，减少阶段切换的开销。
- 缓存复用：优化KV Cache的管理策略（如LRU或多级缓存），在单一设备上高效复用缓存。

#### 3.2 PD聚合的技术实现

PD聚合的实现主要依赖以下技术：

1. 多Token预测（MTP）：
   - 在Prefill阶段预测多个后续Token，减少Decode阶段的迭代次数。
   - 例如，DeepSeek-V3通过将MTP与主模型融合调度，在Prefill节点同时处理MTP的预填充任务，显著降低TPOT。
2. 智能调度：
   - 通过长度预测模块，动态调整Prefill和Decode的资源分配。例如，短序列请求可优先分配给Decode实例，长序列请求则分配更多Prefill资源。
   - 昇腾CANN 8.0的LLM-DataDist组件通过高效调度实现PD聚合，降低部署成本。
3. KV Cache优化：
   - 采用多级缓存策略（如SGLang的PD分离部署示例），在Prefill和Decode阶段之间高效复用KV Cache。
   - 通过压缩或量化KV Cache（如Int8量化），降低显存占用和访存延迟。

#### 3.3 PD聚合的开源项目与案例

1. SGLang：
   - SGLang是一个支持PD分离与聚合的推理框架，通过多级KV Cache策略优化资源利用率。
   - 特点：支持动态Batch Size调整，适应不同请求负载；通过量化技术（如Int8）降低显存占用。
   - 开源地址：https://github.com/sgl-project/sglang
2. 百舸平台：
   - 百度智能云的百舸平台支持PD分离与聚合部署DeepSeek R1模型，通过AIAK推理加速镜像实现高效推理。
   - 特点：支持1:1的Prefill-to-Decode部署，灵活分配算力资源，显著提升推理效率。

3.4 PD聚合的优势与挑战优势：

- 低资源场景适用性：PD聚合适合资源受限的场景，通过合并计算逻辑减少硬件需求。
- 低延迟：通过MTP和智能调度，减少阶段切换开销，降低整体推理延迟。
- 易部署：相比PD分离，PD聚合对网络硬件要求较低，部署复杂度较小。

挑战：

- 优化空间有限：相比PD分离，PD聚合的吞吐量提升幅度较小，难以应对超大规模请求。
- 复杂调度逻辑：智能调度器的设计需考虑多种请求场景，增加开发难度。

------

### 四、PD优化技术的实际应用与未来趋势

#### 4.1 实际应用案例

1. Kimi智能助手：
   - 月之暗面的Kimi智能助手基于Mooncake架构，通过PD分离显著提升推理吞吐量，同时降低成本。
   - 应用场景：智能客服、内容生成、知识管理等。
2. DeepSeek系列模型：
   - DeepSeek通过PD分离和MTP技术，在H800集群上实现高吞吐量推理，广泛应用于第三方评测和企业级部署。
3. NVIDIA与PyTorch：
   - NVIDIA基于PD分离技术孵化下一代LLM服务系统，结合高性能网络（如NVLink）优化KV Cache传输效率。

#### 4.1.1 工程落地案例：GMI Cloud全球化高性能分布式推理服务实践

在AI应用全球化的趋势下，企业面临用户跨地域分布、推理请求高并发、服务稳定性等多重挑战。GMI Cloud（**一家AI Native云服务厂商**）的工程实践为PD分离与聚合技术在生产环境的落地提供了宝贵经验。

**1. 全栈推理服务架构**

GMI Cloud构建了一套覆盖IaaS和MaaS层的全栈技术架构，以应对复杂的生产环境需求：
- **底层IaaS**：整合了GPU计算、存储和网络资源，为上层服务提供统一的资源池。特别是在存储层，通过与JuiceFS（目前我们的平台的存储分层加速使用了Alluxio)等技术合作，实现了存储池的全球化布局（如北美、亚太、欧洲镜像），支持KV Cache的跨区域高效同步，为分钟级拉起新集群提供了基础。
- **上层MaaS**：提供GMI云推理引擎平台（<u>Inference Engine</u>），其核心组件体现了PD分离的设计思想：
    - **API网关与负载均衡**：处理请求验证、限流，并针对**Prefill和Decode阶段实现独立的负载均衡**。其调度层支持**KV Aware路由策略**，通过匹配Prompt哈希值和前缀Token ID，将请求路由到已缓存相应KV的节点，显著提升缓存命中率。
    - **弹性工作负载**：支持以PD分离架构运行工作负载，并能通过**基于Karmada的混合云弹性部署方案**（目前我们的平台也是使用的Karmada)，整合客户自有算力与云端算力池，实现分钟级的资源扩缩容，有效应对突发流量高峰。
    - **存储层优化与冷启动加速**：支持**高频Prompt持久化**(这一方案值得借鉴）。在AI Agent等场景中，系统自动对**高频Prompt进行哈希和统计，并将其KV Cache持久化存储**。新请求若命中，可直接复用缓存，免去Prefill计算，实现了“<u>以存储换计算</u>”的优化。这种跨集群的KV Cache共享机制，极大降低了新建集群的冷启动时间和计算负载。

**2. 开发者友好的推理调优器：Benchmark**

为了解决企业在模型选型、资源预估和参数调优中面临的痛点（如“A100迁到H100需要多少卡？”、“FP16降到FP8能省多少成本？”），GMI Cloud推出了推理引擎调优工具Benchmark。该工具摒弃了传统依赖工程师手动测试的低效方法，实现了自动化、系统化的性能评测。

- **核心功能**：
    - **按需引擎管理**：无需预先申请和占用GPU资源，即可按需启动、停止和管理多种推理引擎（如vLLM, SGLang）的测试实例。
    - **全面的指标采集与存储**：自动收集TTFT、TPOT、输入/输出吞吐量、百万Token成本等关键指标，并统一存储管理测试结果。
    - **简化测试流程**：通过YAML文件定义测试配置，系统可根据行业经验自动优化评估顺序，帮助用户快速找到最优的引擎和参数组合。
    - **可视化分析**：提供交互式仪表盘，方便用户追踪测试进度、分析历史结果，并分享最优配置。

GMI Cloud的实践表明，**成功的推理优化不仅依赖于先进的PD分离/聚合算法，更需要一个集资源弹性调度、智能缓存管理、自动化性能评测于一体的全栈工程体系来支撑**。

#### 4.2 未来趋势

这是我和团队经过调研分析与测试以后对于技术方向的一些见解，供大家参考：

1. 标准化与开源生态：
   - 随着Mooncake、DistServe等项目的开源，PD分离架构有望形成标准化方案，推动推理框架的生态建设。
   - 未来可能出现更多支持PD分离与聚合的开源项目，促进技术平权。
2. 硬件协同优化：
   - PD优化将进一步与硬件设计结合，例如采用高带宽内存（HBM）或专用AI芯片（如NVIDIA H200）优化Decode阶段的访存效率。
3. 智能化调度：
   - **基于强化学习的调度器将成为趋势**（这一方向值得大家一直关注），通过实时分析请求负载和序列长度，动态优化Prefill和Decode的资源分配。

#### 4.3 高频Prompt的专门优化技术

在生产环境中，特别是在AI Agent、RAG（检索增强生成）和固定场景对话机器人等应用中，Prompt往往具有高度的重复性或结构相似性。针对这类高频Prompt进行专门优化，是提升系统吞吐量和降低延迟的关键策略。其核心思想是**避免对相同的输入重复执行计算量巨大的Prefill阶段**。

主要的实现技术可以分为两大类：**KV Cache的持久化与复用** 和 **前缀缓存（Prefix Caching）**。

**1. 核心技术：KV Cache 持久化与复用 (KV Cache Persistence and Reuse)**

这是最主流、最直接的高频Prompt优化技术。其基本原理是将高频Prompt在首次计算后生成的KV Cache保存（持久化）下来。当后续再次遇到完全相同的Prompt时，系统直接加载已保存的KV Cache，跳过整个Prefill阶段，直接进入Decode阶段。

- **实现流程**：
    1.  **Prompt识别与哈希**：对完整的Prompt字符串进行哈希计算（如SHA256）以生成唯一标识。
    2.  **KV Cache存储**：将计算出的KV Cache存入存储系统。根据需求可选择：
        - **内存缓存**：速度最快，但容量有限且易丢失。
        - **分布式缓存（如Redis）**：可跨节点共享，容量更大，但有网络延迟。
        - **持久化存储（如SSD）**：容量最大，可永久保存，用于跨集群的冷启动优化，实现“以存储换计算”。
    3.  **缓存命中与路由**：系统使用哈希值查找缓存。若命中，则直接加载KV Cache进入Decode阶段；若未命中，则正常执行Prefill，并根据LFU/LRU等淘汰策略决定是否将新结果存入缓存。

**2. 高级技术：前缀缓存 (Prefix Caching)**

前缀缓存是KV Cache缓存技术的一个更强大的变种，它解决了“Prompt不完全相同，但有很长公共前缀”的问题，在RAG和多轮对话场景中尤其有效。

- **基本原理**：许多Prompt由一个固定的“系统部分”（如指令、背景知识）和一个变化的“用户部分”（如具体问题）组成。前缀缓存技术只缓存这个公共前缀的KV Cache。

  许多应用场景中，Prompt由一个固定的“系统部分”（如系统指令、角色设定、背景知识）和一个变化的“用户部分”（如用户提
  问）组成。例如：

  "你是一个专业的AWS云架构师。请根据以下背景知识回答用户问题。背景知识：[...大量从文档库检索到的文本...]。现在，
  请回答这个问题：[...用户的具体问题...]"

  在这个例子中，从“你是一个...”到“...请回答这个问题：”这部分是公共前缀，而用户的具体问题是变化的部分。前缀缓存技
  术就是只缓存这个公共前缀的KV Cache。

- **实现方式**：
    1.  **前缀识别**：通过模板匹配或字符串比较识别出请求中的公共前缀。
    2.  **部分Prefill**：当新请求到达时，系统加载已匹配前缀的KV Cache，然后**只对Prompt剩余的不同部分执行Prefill计算**，并将新生成的KV Cache追加到已加载的缓存后面。

前缀缓存极大地提高了缓存的复用率，显著降低了平均Prefill计算量。现代推理引擎（如vLLM）已内置对此技术的支持。

**3. 工程实践总结**

一个优秀的推理优化系统通常会**结合使用**这两种技术，并配合智能的调度策略（如GMI Cloud的**KV Aware路由**，将请求优先路由到已缓存相应KV Cache的GPU节点），以最大限度地提升高频和结构化Prompt场景下的推理效率。

------

### 五、总结

PD分离与PD聚合是大模型推理优化的两大核心策略。PD分离通过解耦Prefill和Decode阶段，针对各自特性独立优化资源分配，显著提升吞吐量和降低延迟，适合大规模、高性能场景；PD聚合通过合并计算逻辑和智能调度，降低资源需求和部署复杂度，适用于资源受限场景。开源项目如Mooncake、DistServe和SGLang为PD优化的研究与应用提供了重要支持，未来随着硬件发展和生态建设，PD优化技术将在大模型推理中发挥更大作用。

------

### 六、工程决策：PD分离 vs. PD聚合

在实际落地时，选择PD分离还是PD聚合，并非一个普适性的“好坏”问题，而是一个高度依赖具体工程环境的权衡。以下是一个帮助决策的实践框架。

**黄金法则**

- **选择PD分离**：旨在实现规模化部署下的**最大吞吐量**和**成本效益**。可将其类比为“工厂流水线”，最适合需要同时为大量用户提供服务的高并发生产级系统。
- **选择PD聚合**：旨在在资源受限或高交互、低并发任务中实现**最低延迟**。可将其类比为“大师工作坊”，最适合实时聊天机器人、单用户体验或边缘计算部署。

**工程决策框架：关键考量因素**

1.  **主要性能目标（吞吐量 vs. 延迟）**
    -   **高吞吐量**：选择**PD分离**。通过为Prefill和Decode创建专门的Worker池，可以消除瓶颈，最大化系统利用率。
    -   **低延迟（特别是TTFT）**：强烈考虑**PD聚合**。在聚合模式下，KV Cache已在GPU内存中，免去了网络传输的开销，使Decode阶段能近乎瞬时启动，对实时交互至关重要。

2.  **典型工作负载（Prompt与生成长度）**
    -   **长Prompt，短生成（如RAG、摘要）**：**PD分离**是明确的赢家。Prefill是主要成本，可为其分配专门的计算优化GPU池。
    -   **短Prompt，长生成（如代码生成）**：**PD分离**依然高效。少数Prefill节点可以服务大量Decode节点，确保对显存带宽要求高的Decode节点始终繁忙。
    -   **短Prompt，短生成（如交互式聊天）**：**PD聚合**是黄金场景。分离和传输小型KV Cache的开销可能超过其收益，聚合模式能提供更流畅的响应。

3.  **硬件与预算限制**
    -   **大型高端集群（如带NVLink的H100）**：完美适配**PD分离**。高速互连能最大限度减少KV Cache传输开销。
    -   **资源有限（如单机、少数GPU、慢速网络）**：**PD聚合**是务实之选。它更简单，能更高效地利用有限的硬件。
    -   **成本敏感型运营**：在规模化部署下，**PD分离**通常能带来更低的每Token成本，因为它提高了硬件的整体利用率。

4.  **架构复杂度**
    -   **PD分离**是一个分布式系统，需要复杂的调度器、强大的网络和更精细的监控，对团队工程能力要求更高。
    -   **PD聚合**是一个单体部署，在部署、管理和调试上要简单得多，适合需要快速迭代或团队规模较小的场景。

**决策速查表**

| 因素 | 选择PD分离，如果... | 选择PD聚合，如果... |
| :--- | :--- | :--- |
| **主要目标** | 您需要**最高的吞吐量**（req/sec）。 | 您需要**最低的延迟**（TTFT）。 |
| **工作负载** | 您的Prompt很长（RAG）**或**生成很长。 | 您的Prompt和生成都很短且需要交互（聊天）。 |
| **硬件规模** | 您拥有带高速网络的**大型集群**（NVLink）。 | 您只有**单台服务器、少数GPU**或慢速网络。 |
| **复杂度** | 您的团队能管理一个**复杂的分布式系统**。 | 您需要一个**简单、易于管理**的部署方案。 |

**最终建议**：首先对您的应用进行性能剖析，了解工作负载特性和瓶颈。对于大规模服务，在**PD分离**上的工程投入几乎肯定会得到回报。对于延迟敏感的小型应用，**PD聚合**是通往生产环境更安全、更快捷的路径。

参考文献：

通过结合理论分析与实际案例，本文为大模型推理优化提供了全面的技术视角，希望对相关研究和应用提供参考。